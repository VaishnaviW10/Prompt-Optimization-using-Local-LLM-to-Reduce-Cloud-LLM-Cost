# Prompt-Optimization-using-Local-LLM-to-Reduce-Cloud-LLM-Cost
Designed a deterministic prompt optimization system to reduce LLM token usage while preserving semantic intent. Built a state-driven LangGraph pipeline using Ollama (Phi-3 Mini) for prompt optimization, tiktoken for token comparison, and a downstream LLM to generate final responses.
